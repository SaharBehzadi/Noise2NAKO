{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predicting Mortality across Germany wih different AI Methods\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import impute\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import dbf\n",
    "from pygam import GAM, s, f, LinearGAM\n",
    "import xgboost\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl\n",
    "from pprint import pprint\n",
    "from stabilityselection import Colinearity_Remover\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir(\"N:/WG_ENRI/20_projects/Noise2NAKO/04_data/grid_prediction/3_output\")\n",
    "grid_path = \"N:/PROJECTS/ENVGEO_DB/1_GIS_data_orig/2_administrative_units/BKG/INSPIRE_geogitter/DE_Grid_ETRS89-LAEA/data/DE_Grid_ETRS89-LAEA_5km.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read training and test data\n",
    "data_training_path = 'Prediction/Reduced_data_SSel/Reduced_data_SSel_Training_data.csv'\n",
    "data_test_path = 'Prediction/Reduced_data_SSel/Reduced_data_SSel_Test_data.csv'\n",
    "data_training = pd.read_csv(data_training_path, sep=',')\n",
    "data_test = pd.read_csv(data_test_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguish between response and input variables and make training and test data \n",
    "# remove unnecessary features 'id', 'x_sw', 'y_sw','x_mp', 'y_mp'\n",
    "\n",
    "output_variable = 'cvd_mortality_17'\n",
    "# 0_original_data\n",
    "# 1_standard_scaled_data\n",
    "# 2_minmax_scaled_data\n",
    "output_path = 'CVD Mortality Rate 2017_allExposures_XY/Reduced_data_SSel/0_original_data'\n",
    "\n",
    "# output_variable = 'ks_Mortality_17'\n",
    "# output_path = 'Total Mortality Rate 2017 incl XY'\n",
    "\n",
    "output_path = '../5_results/' + output_path + '/'\n",
    "coord = True\n",
    "\n",
    "y = data_training[output_variable]\n",
    "if coord:\n",
    "    X = data_training.drop([output_variable, 'id'], axis = 1)   \n",
    "else:\n",
    "    X = data_training.drop([output_variable, 'id', 'x_sw', 'y_sw','x_mp', 'y_mp'], axis = 1)\n",
    "    \n",
    "pred_y = data_test[output_variable]\n",
    "if coord:\n",
    "    pred_X = data_test.drop([output_variable, 'id'], axis = 1)\n",
    "else:\n",
    "    pred_X = data_test.drop([output_variable, 'id', 'x_sw', 'y_sw','x_mp', 'y_mp'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_grid_serach(method):\n",
    "    \n",
    "    # Create a parameter grid\n",
    "\n",
    "    if (method == 'RF'):\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        pprint(random_grid)\n",
    "        \n",
    "    elif (method == 'AdaB'):\n",
    "        # Number of trees in AdaBoost\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # The loss function to use when updating the weights after each boosting iteration\n",
    "        loss = ['linear', 'square', 'exponential']\n",
    "        # Weight applied to each regressor at each boosting iteration\n",
    "        learning_rate = [float(x) for x in np.linspace(0.1, 2, num = 20)]\n",
    "        # Create the random grid\n",
    "        random_grid = {'learning_rate': learning_rate,\n",
    "                       'loss': loss,\n",
    "                       'n_estimators': n_estimators}\n",
    "        pprint(random_grid)\n",
    "        \n",
    "    elif (method == 'XGBoost'):\n",
    "        # Number of gradient boosted trees\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # Boosting learning rate\n",
    "        learning_rate = [float(x) for x in np.linspace(0.01, 0.2, num = 20)]\n",
    "        # Create the random grid\n",
    "        random_grid = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': learning_rate,\n",
    "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "           'n_estimators': n_estimators}\n",
    "        pprint(random_grid)\n",
    "        \n",
    "    return random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch_Param_Tuning(model, method, itr, X, y, n_cv = 3, verbose = 2, rand_state = 42, jobs = -1):\n",
    "    \n",
    "    grid = Create_grid_serach(method)\n",
    "    random_model = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = itr, cv = n_cv,\n",
    "                                      verbose=2, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    random_model.fit(X, y)\n",
    "    \n",
    "    return random_model.best_estimator_, random_model.best_params_, random_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_Param_Tuning(model, grid, X, y, n_cv = 3, verbose = 2, jobs = -1):\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = grid, cv = n_cv, n_jobs = jobs, verbose = verbose)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(method, X_train, X_test, y_train, y_test, pred_X, pred_y, output_variable, output_path, data_training, data_test, grid_5km_shp):\n",
    "    \n",
    "    '''Prediction Function'''\n",
    "    \n",
    "    if method == 'LR':\n",
    "        model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "    elif method == 'LR_Ridge':\n",
    "        model = linear_model.RidgeCV(cv = 5).fit(X_train, y_train)\n",
    "    elif method == 'LR_Lasso':\n",
    "        model = linear_model.LassoCV(cv = 5, random_state=0).fit(X_train, y_train)\n",
    "    elif method == 'LR_Elastic':\n",
    "        model = linear_model.ElasticNetCV(cv = 5, random_state=0).fit(X_train, y_train)\n",
    "    elif method == 'GAM':\n",
    "        model = GAM().fit(X_train, y_train)\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestRegressor()\n",
    "        model, parameters, all_models = RandomSearch_Param_Tuning(model, method, 100, X_train, y_train)\n",
    "    elif method == 'AdaB':\n",
    "        model = AdaBoostRegressor()\n",
    "        model, parameters, all_models = RandomSearch_Param_Tuning(model, method, 100, X_train, y_train)\n",
    "    else:\n",
    "        model = xgboost.XGBRegressor()\n",
    "        model, parameters, all_models = RandomSearch_Param_Tuning(model, method, 100, X_train, y_train)\n",
    "    \n",
    "    print(method + ' is running ... ')\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred = model.predict(pred_X)\n",
    "    y_pred = pd.DataFrame(y_pred, columns = [output_variable])\n",
    "\n",
    "    result = {'Method': method, 'MSE_train': mean_squared_error(y_train, y_pred_train),\n",
    "                            'MAE_train': mean_absolute_error(y_train, y_pred_train), 'R_2_train': r2_score(y_train, y_pred_train),\n",
    "                           'MSE_val': mean_squared_error(y_test, y_pred_test), 'MAE_val': mean_absolute_error(y_test, y_pred_test),\n",
    "                           'R_2_val': r2_score(y_test, y_pred_test), 'MSE_pred': mean_squared_error(pred_y, y_pred), 'MAE_pred': mean_absolute_error(pred_y, y_pred),\n",
    "                           'R_2_pred': r2_score(pred_y, y_pred)}\n",
    "    \n",
    "#     Merge dataframes to make the final output dataset \n",
    "    data_result= pd.DataFrame()\n",
    "    data_result = data_result.append(data_training)\n",
    "    data_result = data_result.append(pd.concat([data_test.drop([output_variable], axis = 1),pd.DataFrame(y_pred)],axis=1))\n",
    "    data_result = data_result[['id', output_variable]]\n",
    "    data_result.to_csv((output_path + method + '/prediction_result_' + method + '.csv'), sep=',', index=False)\n",
    "    \n",
    "#     Merge dataframes to make the Ground Truth\n",
    "    GT = pd.DataFrame()\n",
    "    GT = GT.append(data_training)\n",
    "    GT = GT.append(pd.concat([data_test.drop([output_variable], axis = 1),pd.DataFrame(pred_y)],axis=1))\n",
    "    GT = GT[['id', output_variable]]\n",
    "    GT.to_csv((output_path + 'GT/GT.csv'), sep=',', index=False)   \n",
    "    \n",
    "#     Make the difference output\n",
    "    data_diff_result= pd.DataFrame()\n",
    "    data_diff_result = data_diff_result.append(data_training)\n",
    "    data_diff_result[output_variable] = 0\n",
    "    pred_variable = 'pred'+output_variable\n",
    "    data_test = pd.concat([data_test, y_pred.rename(columns={output_variable: pred_variable})], axis=1)\n",
    "    data_test[output_variable] = data_test[output_variable] - data_test[pred_variable]\n",
    "    data_diff_result = data_diff_result.append(data_test.drop([pred_variable], axis = 1))\n",
    "    data_diff_result = data_diff_result[['id', output_variable]]\n",
    "    data_diff_result.to_csv((output_path + 'diff_' + method + '/prediction_result_diff_' + method + '.csv'), sep=',', index=False)\n",
    "    \n",
    "#     Make shp files\n",
    "    merged_data = grid_5km_shp.merge(data_result, left_on=\"id\", right_on=\"id\")\n",
    "#     merged_data_GT = grid_5km_shp.merge(GT, left_on=\"id\", right_on=\"id\")\n",
    "    merged_data_diff = grid_5km_shp.merge(data_diff_result, left_on=\"id\", right_on=\"id\")\n",
    "#     save the GeoDataFrame\n",
    "    merged_data.to_file(driver = 'ESRI Shapefile', filename= output_path + method + '/' + method + '_shape.shp')\n",
    "#     merged_data.to_file(driver = 'ESRI Shapefile', filename= output_path + 'GT/GT_shape.shp')\n",
    "    merged_data_diff.to_file(driver = 'ESRI Shapefile', filename= output_path + 'diff_'+ method + '/diff_' + method + '_shape.shp')\n",
    "    \n",
    "    return result, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR is running ... \n",
      "LR is done!\n",
      "\n",
      "LR_Ridge is running ... \n",
      "LR_Ridge is done!\n",
      "\n",
      "LR_Lasso is running ... \n",
      "LR_Lasso is done!\n",
      "\n",
      "LR_Elastic is running ... \n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(columns=['Method', 'MSE_train', 'MAE_train', 'R_2_train', 'MSE_val', 'MAE_val', 'R_2_val', 'MSE_pred', 'MAE_pred', 'R_2_pred'])\n",
    "methods = ['LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'RF', 'AdaB', 'XGBoost']\n",
    "# methods = ['RF', 'AdaB', 'XGBoost']\n",
    "preds = pd.DataFrame()\n",
    "preds['GT'] = pred_y\n",
    "\n",
    "grid_5km_shp = gpd.read_file(grid_path)\n",
    "\n",
    "for i in methods:\n",
    "    ToAppend, y_pred = Prediction(i, X_train, X_test, y_train, y_test, pred_X, pred_y, output_variable,\n",
    "                                 output_path, data_training, data_test, grid_5km_shp)\n",
    "    print(i + ' is done!\\n')\n",
    "    result = result.append(ToAppend, ignore_index=True)\n",
    "    preds[i] = y_pred\n",
    "    \n",
    "result.to_csv(output_path + 'PredictionTask_results_allMethods.csv', index=False, sep=',')\n",
    "result.round(decimals=3).to_csv(output_path + 'PredictionTask_results_allMethods_round.csv', index=False, sep=',')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def single_map_plot(output_path, method, show_map = False):\n",
    "    \n",
    "#     data_shp = gpd.read_file(output_path + method + '/' + method + '_shape.shp')\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(10,15))\n",
    "#     data_shp.plot(column='cvd_mortal', ax = ax, legend = True, legend_kwds={'loc': 'lower right'}, cmap = plt.cm.get_cmap('magma_r'), scheme='user_defined', classification_kwds={'bins':[3, 4, 5, 6, 8, 10]})\n",
    "#     ax.set_axis_off()\n",
    "#     ax.set_title(method, fontsize = 20)\n",
    "#     plt.savefig(output_path + method + '/' + method + '_plot')\n",
    "#     if (show_map == True):\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in methods:\n",
    "#     single_map_plot(output_path, i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_maps_plot(output_path, methods, number_of_columns, color_map = 'YlGn', file_name = 'all_methods_maps', bins = None, output_var = None):\n",
    "           \n",
    "    # Subplots are organized in a Rows x Cols Grid\n",
    "    # Tot and Cols are known\n",
    "    Tot = len(methods)\n",
    "    Cols = number_of_columns\n",
    "\n",
    "    # Compute Rows required\n",
    "    Rows = Tot // Cols \n",
    "    Rows += Tot % Cols\n",
    "\n",
    "    # Create a Position index\n",
    "    Position = range(1,Tot + 1)\n",
    "    \n",
    "    # Set the size of the figure\n",
    "    fig_size_width = 5 * min(number_of_columns, len(methods))\n",
    "    fig_size_heigth = 7 * np.ceil((len(methods) / number_of_columns))\n",
    "    fig = plt.figure(1, figsize=(fig_size_width, fig_size_heigth))\n",
    "    first_plot = 1\n",
    "    \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    \n",
    "    for k in range(Tot):\n",
    "        \n",
    "        data_shp = gpd.read_file(output_path + methods[k] + '/' + methods[k] + '_shape.shp')\n",
    "        \n",
    "        # if not defined, set the output variable to group the maps by\n",
    "        if not output_var:\n",
    "            for i in data_shp.columns:\n",
    "                if 'mortal' in i:\n",
    "                    output_var = i\n",
    "                    \n",
    "        # add every single subplot to the figure with a for loop\n",
    "        ax = fig.add_subplot(Rows,Cols,Position[k])\n",
    "        \n",
    "        if (first_plot == 1 and bins): \n",
    "            data_shp.plot(column = output_var, ax = ax, legend = True, legend_kwds={'loc': 'lower right', 'bbox_to_anchor': (0, 1)}, cmap = plt.cm.get_cmap(color_map), scheme='user_defined', classification_kwds={'bins': bins})\n",
    "            first_plot = 2\n",
    "        elif (first_plot == 1 and not bins):\n",
    "            first_plot = 2\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "            data_shp.plot(column = output_var, ax = ax, legend = True, cax = cax, cmap = plt.cm.get_cmap(color_map))\n",
    "        elif(first_plot != 1 and bins):\n",
    "            data_shp.plot(column = output_var, ax = ax, cmap = plt.cm.get_cmap(color_map), scheme='user_defined', classification_kwds={'bins':bins})\n",
    "        else:\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "            data_shp.plot(column = output_var, ax = ax, cax = cax, legend = True, cmap = plt.cm.get_cmap(color_map))\n",
    "            \n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(methods[k] , fontsize= 16)\n",
    "    \n",
    "    plt.savefig(output_path + file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GT = pd.read_csv(output_path + 'GT/GT.csv', sep=',')\n",
    "merged_data_GT = grid_5km_shp.merge(GT, left_on=\"id\", right_on=\"id\")\n",
    "merged_data_GT.to_file(driver = 'ESRI Shapefile', filename= output_path + 'GT/GT_shape.shp')\n",
    "\n",
    "to_plot = ['GT', 'LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'RF', 'AdaB', 'XGBoost']\n",
    "# all_maps_plot(output_path, to_plot, 3, color_map= 'magma_r', file_name= 'Plots/all_methods_maps', bins= [0.2, 0.4, 0.6, 0.8, 1], output_var='cvd_mortal')\n",
    "# all_maps_plot(output_path, to_plot, 3, color_map= 'magma_r', file_name= 'Plots/all_methods_maps', bins= [-2, -1, 0, 1, 2], output_var='cvd_mortal') \n",
    "all_maps_plot(output_path, to_plot, 3, color_map= 'magma_r', file_name= 'Plots/all_methods_maps', bins= [3, 4, 5, 6, 8, 10], output_var='cvd_mortal') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_to_plot = ['LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'RF', 'AdaB', 'XGBoost']\n",
    "for i in range(len(methods_to_plot)):\n",
    "    methods_to_plot[i] = 'diff_' + methods_to_plot[i]\n",
    "# all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps', bins = [-0.25, 0, 0.25, 0.5], output_var='cvd_mortal')\n",
    "# all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps', bins = [-1, 0, 1, 2.5], output_var='cvd_mortal')\n",
    "all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps', bins = [-1, 0, 1, 2.5, 3.5], output_var='cvd_mortal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make performance correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# sns.set(style=\"ticks\", color_codes=True)\n",
    "# sns.pairplot(preds, corner=True, kind=\"reg\", plot_kws={'line_kws':{'color':'red'}})\n",
    "# plt.savefig(output_path + 'Plots/Performance_correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = preds.corr(method='spearman')\n",
    "# upper_corr_matrix = np.tril(correlation_matrix)\n",
    "# plt.figure()\n",
    "# sns.heatmap(correlation_matrix, cbar=True, fmt='.1f', annot=True, cmap='Reds', mask = upper_corr_matrix )\n",
    "# plt.savefig(output_path + 'Plots/Performance_correlation_Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_colname(x, **kws):\n",
    "  ax = plt.gca()\n",
    "  ax.annotate(x.name, xy=(0.05, 0.9), xycoords=ax.transAxes,\n",
    "              fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrdot(*args, **kwargs):\n",
    "    corr_r = args[0].corr(args[1], 'spearman')\n",
    "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    marker_size = abs(corr_r) * 10000\n",
    "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
    "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
    "    font_size = abs(corr_r) * 40 + 5\n",
    "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
    "                ha='center', va='center', fontsize=font_size)\n",
    "plt.figure()\n",
    "sns.set(style='white', font_scale=1.6)\n",
    "g = sns.PairGrid(preds, aspect=1.4, diag_sharey=False)\n",
    "#g.map_lower(sns.regplot, lowess=True, ci=True, line_kws={'color': 'red'})\n",
    "g.map_lower(sns.regplot, scatter_kws={'s':10}, line_kws={'color': 'red'})\n",
    "g.map_diag(sns.distplot, kde_kws={'color': 'red'})\n",
    "g.map_diag(annotate_colname)\n",
    "g.map_upper(corrdot)\n",
    "\n",
    "plt.savefig(output_path + 'Plots/scatter_corr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
