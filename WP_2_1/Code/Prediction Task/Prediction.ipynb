{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predicting Mortality across Germany wih different AI Methods\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import impute\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import dbf\n",
    "from pygam import GAM, s, f, LinearGAM\n",
    "import xgboost\n",
    "import joblib\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib as mpl\n",
    "from pprint import pprint\n",
    "from stabilityselection import Colinearity_Remover\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir(\"N:/WG_ENRI/20_projects/Noise2NAKO/04_data/grid_prediction/3_output\")\n",
    "grid_path = \"N:/PROJECTS/ENVGEO_DB/1_GIS_data_orig/2_administrative_units/BKG/INSPIRE_geogitter/DE_Grid_ETRS89-LAEA/data/DE_Grid_ETRS89-LAEA_5km.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read training and test data\n",
    "data_training_path = 'Prediction_data/Reduced_data_imp/Reduced_data_imp_SSel_Training_data.csv'\n",
    "data_test_path = 'Prediction_data/Reduced_data_imp/Reduced_data_imp_SSel_Test_data.csv'\n",
    "data_training = pd.read_csv(data_training_path, sep=',')\n",
    "data_test = pd.read_csv(data_test_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguish between response and input variables and make training and validation data \n",
    "\n",
    "output_variable = 'cvd_mortality_17'\n",
    "# 0_original_data\n",
    "# 1_standard_scaled_data\n",
    "# 2_minmax_scaled_data\n",
    "output_path = 'CVD Mortality Rate 2017_allExposures_XY/Reduced_data_imp/0_original_data'\n",
    "\n",
    "output_path = '../5_results/' + output_path + '/'\n",
    "y = data_training[output_variable]\n",
    "X = data_training.drop([output_variable, 'id'], axis = 1)   \n",
    "y_test = data_test[output_variable]\n",
    "X_test = data_test.drop([output_variable, 'id'], axis = 1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_grid_serach(method):\n",
    "    \n",
    "    # Create a parameter grid\n",
    "\n",
    "    if (method == 'RF'):\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        \n",
    "    elif (method == 'AdaB'):\n",
    "        # Number of trees in AdaBoost\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # The loss function to use when updating the weights after each boosting iteration\n",
    "        loss = ['linear', 'square', 'exponential']\n",
    "        # Weight applied to each regressor at each boosting iteration\n",
    "        learning_rate = [float(x) for x in np.linspace(0.1, 2, num = 20)]\n",
    "        # Create the random grid\n",
    "        grid = {'learning_rate': learning_rate,\n",
    "                       'loss': loss,\n",
    "                       'n_estimators': n_estimators}\n",
    "        \n",
    "    elif (method == 'XGBoost'):\n",
    "        # Number of gradient boosted trees\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "        # Boosting learning rate\n",
    "        learning_rate = [float(x) for x in np.linspace(0.01, 0.2, num = 20)]\n",
    "        # Create the random grid\n",
    "        grid = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': learning_rate,\n",
    "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "           'n_estimators': n_estimators}\n",
    "        \n",
    "    elif (method == 'SVM'):\n",
    "        # C parameter\n",
    "        C = [0.1, 1, 10, 100]\n",
    "        # kernel\n",
    "        kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "        # Create the random grid\n",
    "        grid = { \n",
    "            'C': C,\n",
    "           'kernel': kernel\n",
    "                      }\n",
    "        \n",
    "    elif (method == 'KNN'):\n",
    "        # Algorithm used to compute the nearest neighbors\n",
    "        algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "        n_beighbors = list(range(1,30))\n",
    "#         Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1),\n",
    "#         and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "        p = [1, 2]\n",
    "#         Weight function used in prediction\n",
    "        weight = ['uniform', 'distance']\n",
    "#         Leaf size passed to BallTree or KDTree. \n",
    "        leaf_size = list(range(1,50))\n",
    "        grid = {\n",
    "            'algorithm': algorithm,\n",
    "            'leaf_size': leaf_size,\n",
    "                       'n_neighbors': n_beighbors,\n",
    "                       'p': p,\n",
    "                       'weights': weight\n",
    "                      }\n",
    "    elif (method == 'MLP'):\n",
    "        \n",
    "        grid = {\n",
    "            'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30), (100,)],\n",
    "            'max_iter': [50, 100],\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam', 'invscaling'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "            }\n",
    "    \n",
    "    pprint(grid)\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch_Param_Tuning(model, method, itr, X, y, n_cv = 3, verbose = 2, rand_state = 42, jobs = -1):\n",
    "    \n",
    "    print('Runnig Random Search for parameter Tuning when Scoring is R-squared ..... ')\n",
    "    \n",
    "    grid = Create_grid_serach(method)\n",
    "    random_model = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = itr, cv = n_cv,\n",
    "                                      scoring= 'r2', verbose=2, random_state=42, n_jobs = -1)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    random_model.fit(X, y)\n",
    "    \n",
    "    return random_model.best_estimator_, random_model.best_params_, random_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_Param_Tuning(model, method, X, y, grid = None, n_cv = 3, verbose = 2, jobs = -1):\n",
    "    \n",
    "    print('Runnig Grid Search for parameter Tuning  when Scoring is R-squared ..... ')\n",
    "    \n",
    "    if not grid:\n",
    "        grid = Create_grid_serach(method)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = grid, cv = n_cv, scoring= 'r2', n_jobs = jobs,\n",
    "                               verbose = verbose)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_fit_model(method, X_train, y_train):\n",
    "    \n",
    "    if method == 'LR':\n",
    "        model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "    elif method == 'LR_Ridge':\n",
    "        model = linear_model.RidgeCV(cv = 5).fit(X_train, y_train)\n",
    "    elif method == 'LR_Lasso':\n",
    "        model = linear_model.LassoCV(cv = 5, random_state=0).fit(X_train, y_train)\n",
    "    elif method == 'LR_Elastic':\n",
    "        model = linear_model.ElasticNetCV(cv = 5, random_state=0).fit(X_train, y_train)\n",
    "    elif method == 'GAM':\n",
    "        model = GAM().fit(X_train, y_train)\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestRegressor()\n",
    "    elif method == 'AdaB':\n",
    "        model = AdaBoostRegressor()\n",
    "    elif method == 'XGBoost':\n",
    "        model = xgboost.XGBRegressor()\n",
    "    elif method == 'SVM':\n",
    "        model = SVR()\n",
    "    elif method == 'KNN':\n",
    "        model = KNeighborsRegressor()\n",
    "    elif method == 'MLP':\n",
    "        model = MLPRegressor()\n",
    "    \n",
    "    if method in ['KNN', 'SVM', 'RF', 'AdaB', 'XGBoost', 'MLP']:\n",
    "        model, parameters, all_models = RandomSearch_Param_Tuning(model, method, 100, X_train, y_train)\n",
    "        \n",
    "    print('Parameters for ' + method)\n",
    "    pprint(model.get_params())\n",
    "    \n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    joblib.dump(model, 'trained_models/' + method + '_trained_model.sav')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path, model_name = None):\n",
    "    joblib.dump(model, file_path + model_name + '_trained_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_path, model_name = None):\n",
    "    loaded_model = joblib.load(file_path + model_name + '.sav')\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(method, X_train, X_val, y_train, y_val, X_test, y_test, output_variable, output_path, data_training,\n",
    "               data_test, grid_5km_shp, model):\n",
    "    \n",
    "    '''Prediction Function'''\n",
    "    \n",
    "    print(method + ' is running ... ')\n",
    "    \n",
    "    if not model:\n",
    "        # If no trained model is provided, first, build and train the desired model\n",
    "        model = Build_fit_model(method, X_train, y_train)\n",
    "    \n",
    "    # Validation and Evaluation of the model\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_val)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns = [output_variable])\n",
    "\n",
    "    result = {'Method': method, 'MSE_train': mean_squared_error(y_train, y_pred_train),\n",
    "                            'MAE_train': mean_absolute_error(y_train, y_pred_train), 'R_2_train': r2_score(y_train, y_pred_train),\n",
    "                           'MSE_val': mean_squared_error(y_val, y_pred_test), 'MAE_val': mean_absolute_error(y_val, y_pred_test),\n",
    "                           'R_2_val': r2_score(y_val, y_pred_test), 'MSE_pred': mean_squared_error(y_test, y_pred), 'MAE_pred': mean_absolute_error(y_test, y_pred),\n",
    "                           'R_2_pred': r2_score(y_test, y_pred)}\n",
    "    print(method + ' results:')\n",
    "    pprint(result)\n",
    "    \n",
    "#     Merge dataframes to make the final output dataset \n",
    "    data_result= pd.DataFrame()\n",
    "    data_result = data_result.append(data_training)\n",
    "    data_result = data_result.append(pd.concat([data_test.drop([output_variable], axis = 1),pd.DataFrame(y_pred)],axis=1))\n",
    "    data_result = data_result[['id', output_variable]]\n",
    "    data_result.to_csv((output_path + method + '/prediction_result_' + method + '.csv'), sep=',', index=False)\n",
    "    \n",
    "#     Merge dataframes to make the Ground Truth\n",
    "    GT = pd.DataFrame()\n",
    "    GT = GT.append(data_training)\n",
    "    GT = GT.append(pd.concat([data_test.drop([output_variable], axis = 1),pd.DataFrame(y_test)],axis=1))\n",
    "    GT = GT[['id', output_variable]]\n",
    "    GT.to_csv((output_path + 'GT/GT.csv'), sep=',', index=False)   \n",
    "    \n",
    "#     Make the difference output\n",
    "    data_diff_result= pd.DataFrame()\n",
    "    data_diff_result = data_diff_result.append(data_training)\n",
    "    data_diff_result[output_variable] = 0\n",
    "    pred_variable = 'pred'+output_variable\n",
    "    data_test = pd.concat([data_test, y_pred.rename(columns={output_variable: pred_variable})], axis=1)\n",
    "    data_test[output_variable] = data_test[output_variable] - data_test[pred_variable]\n",
    "    data_diff_result = data_diff_result.append(data_test.drop([pred_variable], axis = 1))\n",
    "    data_diff_result = data_diff_result[['id', output_variable]]\n",
    "    data_diff_result.to_csv((output_path + 'diff_' + method + '/prediction_result_diff_' + method + '.csv'), sep=',', index=False)\n",
    "    \n",
    "#     Make shp files\n",
    "    merged_data = grid_5km_shp.merge(data_result, left_on=\"id\", right_on=\"id\")\n",
    "#     merged_data_GT = grid_5km_shp.merge(GT, left_on=\"id\", right_on=\"id\")\n",
    "    merged_data_diff = grid_5km_shp.merge(data_diff_result, left_on=\"id\", right_on=\"id\")\n",
    "#     save the GeoDataFrame\n",
    "    merged_data.to_file(driver = 'ESRI Shapefile', filename= output_path + method + '/' + method + '_shape.shp')\n",
    "#     merged_data.to_file(driver = 'ESRI Shapefile', filename= output_path + 'GT/GT_shape.shp')\n",
    "    merged_data_diff.to_file(driver = 'ESRI Shapefile', filename= output_path + 'diff_'+ method + '/diff_' + method + '_shape.shp')\n",
    "    \n",
    "    return result, y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_fit_all_methods(methods, X_train, y_train):\n",
    "    \n",
    "    models = []\n",
    "    for method in methods:\n",
    "        # Build and fit the model with the proper data\n",
    "        models.append(Build_fit_model(method, X_train, y_train))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for LR\n",
      "{'copy_X': True,\n",
      " 'fit_intercept': True,\n",
      " 'n_jobs': None,\n",
      " 'normalize': False,\n",
      " 'positive': False}\n",
      "LR is running ... \n",
      "LR results:\n",
      "{'MAE_pred': 0.5318989050517257,\n",
      " 'MAE_train': 0.21260932886123937,\n",
      " 'MAE_val': 0.18935888367889034,\n",
      " 'MSE_pred': 0.5291800503921662,\n",
      " 'MSE_train': 0.08840333437689993,\n",
      " 'MSE_val': 0.06842921529773388,\n",
      " 'Method': 'LR',\n",
      " 'R_2_pred': 0.4767446675638084,\n",
      " 'R_2_train': 0.9151431051926828,\n",
      " 'R_2_val': 0.9328860177182701}\n",
      "LR is done!\n",
      "\n",
      "Parameters for LR_Ridge\n",
      "{'alpha_per_target': False,\n",
      " 'alphas': array([ 0.1,  1. , 10. ]),\n",
      " 'cv': 5,\n",
      " 'fit_intercept': True,\n",
      " 'gcv_mode': None,\n",
      " 'normalize': False,\n",
      " 'scoring': None,\n",
      " 'store_cv_values': False}\n",
      "LR_Ridge is running ... \n",
      "LR_Ridge results:\n",
      "{'MAE_pred': 0.5436238377775211,\n",
      " 'MAE_train': 0.2120058995839067,\n",
      " 'MAE_val': 0.18811564407159898,\n",
      " 'MSE_pred': 0.5467072548060795,\n",
      " 'MSE_train': 0.08890096334825118,\n",
      " 'MSE_val': 0.06774567282467417,\n",
      " 'Method': 'LR_Ridge',\n",
      " 'R_2_pred': 0.4594136983304016,\n",
      " 'R_2_train': 0.9146654393945243,\n",
      " 'R_2_val': 0.9335564222702752}\n",
      "LR_Ridge is done!\n",
      "\n",
      "Parameters for LR_Lasso\n",
      "{'alphas': None,\n",
      " 'copy_X': True,\n",
      " 'cv': 5,\n",
      " 'eps': 0.001,\n",
      " 'fit_intercept': True,\n",
      " 'max_iter': 1000,\n",
      " 'n_alphas': 100,\n",
      " 'n_jobs': None,\n",
      " 'normalize': False,\n",
      " 'positive': False,\n",
      " 'precompute': 'auto',\n",
      " 'random_state': 0,\n",
      " 'selection': 'cyclic',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "LR_Lasso is running ... \n",
      "LR_Lasso results:\n",
      "{'MAE_pred': 0.6935343427767665,\n",
      " 'MAE_train': 0.606895205855962,\n",
      " 'MAE_val': 0.5915373253287366,\n",
      " 'MSE_pred': 0.891386125574435,\n",
      " 'MSE_train': 0.542494803467217,\n",
      " 'MSE_val': 0.5002029490436217,\n",
      " 'Method': 'LR_Lasso',\n",
      " 'R_2_pred': 0.11859386399619165,\n",
      " 'R_2_train': 0.47926823353664416,\n",
      " 'R_2_val': 0.5094111234022228}\n",
      "LR_Lasso is done!\n",
      "\n",
      "Parameters for LR_Elastic\n",
      "{'alphas': None,\n",
      " 'copy_X': True,\n",
      " 'cv': 5,\n",
      " 'eps': 0.001,\n",
      " 'fit_intercept': True,\n",
      " 'l1_ratio': 0.5,\n",
      " 'max_iter': 1000,\n",
      " 'n_alphas': 100,\n",
      " 'n_jobs': None,\n",
      " 'normalize': False,\n",
      " 'positive': False,\n",
      " 'precompute': 'auto',\n",
      " 'random_state': 0,\n",
      " 'selection': 'cyclic',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0}\n",
      "LR_Elastic is running ... \n",
      "LR_Elastic results:\n",
      "{'MAE_pred': 0.6935343768766765,\n",
      " 'MAE_train': 0.6068952135707485,\n",
      " 'MAE_val': 0.5915373487088,\n",
      " 'MSE_pred': 0.891386220716383,\n",
      " 'MSE_train': 0.5424948288023327,\n",
      " 'MSE_val': 0.5002030085951857,\n",
      " 'Method': 'LR_Elastic',\n",
      " 'R_2_pred': 0.11859376991945625,\n",
      " 'R_2_train': 0.4792682092178867,\n",
      " 'R_2_val': 0.5094110649952603}\n",
      "LR_Elastic is done!\n",
      "\n",
      "Parameters for GAM\n",
      "{'callbacks': [Deviance(), Diffs()],\n",
      " 'distribution': NormalDist(),\n",
      " 'fit_intercept': True,\n",
      " 'link': IdentityLink(),\n",
      " 'max_iter': 100,\n",
      " 'terms': s(0) + s(1) + s(2) + s(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9) + s(10) + s(11) + s(12) + s(13) + s(14) + s(15) + s(16) + s(17) + s(18) + s(19) + s(20) + intercept,\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "GAM is running ... \n",
      "GAM results:\n",
      "{'MAE_pred': 0.48353939063234386,\n",
      " 'MAE_train': 0.058728260601501055,\n",
      " 'MAE_val': 0.055437854236875,\n",
      " 'MSE_pred': 0.39149243753615537,\n",
      " 'MSE_train': 0.007165398126244864,\n",
      " 'MSE_val': 0.006750630261553748,\n",
      " 'Method': 'GAM',\n",
      " 'R_2_pred': 0.6128907251937695,\n",
      " 'R_2_train': 0.9931220531517623,\n",
      " 'R_2_val': 0.9933791191701795}\n",
      "GAM is done!\n",
      "\n",
      "Runnig Random Search for parameter Tuning when Scoring is R-squared ..... \n",
      "{'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
      " 'leaf_size': [1,\n",
      "               2,\n",
      "               3,\n",
      "               4,\n",
      "               5,\n",
      "               6,\n",
      "               7,\n",
      "               8,\n",
      "               9,\n",
      "               10,\n",
      "               11,\n",
      "               12,\n",
      "               13,\n",
      "               14,\n",
      "               15,\n",
      "               16,\n",
      "               17,\n",
      "               18,\n",
      "               19,\n",
      "               20,\n",
      "               21,\n",
      "               22,\n",
      "               23,\n",
      "               24,\n",
      "               25,\n",
      "               26,\n",
      "               27,\n",
      "               28,\n",
      "               29,\n",
      "               30,\n",
      "               31,\n",
      "               32,\n",
      "               33,\n",
      "               34,\n",
      "               35,\n",
      "               36,\n",
      "               37,\n",
      "               38,\n",
      "               39,\n",
      "               40,\n",
      "               41,\n",
      "               42,\n",
      "               43,\n",
      "               44,\n",
      "               45,\n",
      "               46,\n",
      "               47,\n",
      "               48,\n",
      "               49],\n",
      " 'n_neighbors': [1,\n",
      "                 2,\n",
      "                 3,\n",
      "                 4,\n",
      "                 5,\n",
      "                 6,\n",
      "                 7,\n",
      "                 8,\n",
      "                 9,\n",
      "                 10,\n",
      "                 11,\n",
      "                 12,\n",
      "                 13,\n",
      "                 14,\n",
      "                 15,\n",
      "                 16,\n",
      "                 17,\n",
      "                 18,\n",
      "                 19,\n",
      "                 20,\n",
      "                 21,\n",
      "                 22,\n",
      "                 23,\n",
      "                 24,\n",
      "                 25,\n",
      "                 26,\n",
      "                 27,\n",
      "                 28,\n",
      "                 29],\n",
      " 'p': [1, 2],\n",
      " 'weights': ['uniform', 'distance']}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Parameters for KNN\n",
      "{'algorithm': 'ball_tree',\n",
      " 'leaf_size': 42,\n",
      " 'metric': 'minkowski',\n",
      " 'metric_params': None,\n",
      " 'n_jobs': None,\n",
      " 'n_neighbors': 29,\n",
      " 'p': 2,\n",
      " 'weights': 'distance'}\n",
      "KNN is running ... \n",
      "KNN results:\n",
      "{'MAE_pred': 0.6503739811074886,\n",
      " 'MAE_train': 0.0,\n",
      " 'MAE_val': 0.0,\n",
      " 'MSE_pred': 0.7172307840703535,\n",
      " 'MSE_train': 0.0,\n",
      " 'MSE_val': 0.0,\n",
      " 'Method': 'KNN',\n",
      " 'R_2_pred': 0.2907993563361305,\n",
      " 'R_2_train': 1.0,\n",
      " 'R_2_val': 1.0}\n",
      "KNN is done!\n",
      "\n",
      "Runnig Random Search for parameter Tuning when Scoring is R-squared ..... \n",
      "{'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "result_allTrain = pd.DataFrame(columns=['Method', 'MSE_train', 'MAE_train', 'R_2_train', 'MSE_val', 'MAE_val', 'R_2_val', 'MSE_pred', 'MAE_pred', 'R_2_pred'])\n",
    "methods = ['LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'KNN', 'SVM', 'RF', 'AdaB', 'XGBoost', 'MLP']\n",
    "preds_allTrain = pd.DataFrame()\n",
    "preds_allTrain['GT'] = y_test\n",
    "models_allTrain = []\n",
    "\n",
    "grid_5km_shp = gpd.read_file(grid_path)\n",
    "\n",
    "for i in methods:\n",
    "    model = Build_fit_model(i, X, y)\n",
    "    models_allTrain.append(model)\n",
    "    ToAppend, y_pred, trained_model = Prediction(i, X, X_val, y, y_val, X_test, y_test, output_variable,\n",
    "                                 output_path, data_training, data_test, grid_5km_shp, model)\n",
    "    print(i + ' is done!\\n')\n",
    "    result_allTrain = result_allTrain.append(ToAppend, ignore_index=True)\n",
    "    preds_allTrain[i] = y_pred\n",
    "    \n",
    "result_allTrain.to_csv(output_path + 'PredictionTask_results_allMethods_allTrain.csv', index=False, sep=',')\n",
    "result_allTrain.round(decimals=3).to_csv(output_path + 'PredictionTask_results_allMethods_round_allTrain.csv', index=False, sep=',')\n",
    "\n",
    "result_allTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['Method', 'MSE_train', 'MAE_train', 'R_2_train', 'MSE_val', 'MAE_val', 'R_2_val', 'MSE_pred', 'MAE_pred', 'R_2_pred'])\n",
    "methods = ['LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'KNN', 'SVM', 'RF', 'AdaB', 'XGBoost', 'MLP']\n",
    "preds = pd.DataFrame()\n",
    "preds['GT'] = y_test\n",
    "models = []\n",
    "\n",
    "grid_5km_shp = gpd.read_file(grid_path)\n",
    "\n",
    "for i in methods:\n",
    "    model = Build_fit_model(i, X_train, y_train)\n",
    "    models.append(model)\n",
    "    ToAppend, y_pred, trained_model = Prediction(i, X_train, X_val, y_train, y_val, X_test, y_test, output_variable,\n",
    "                                 output_path, data_training, data_test, grid_5km_shp, model)\n",
    "    print(i + ' is done!\\n')\n",
    "    result = result.append(ToAppend, ignore_index=True)\n",
    "    preds[i] = y_pred\n",
    "    \n",
    "result.to_csv(output_path + 'PredictionTask_results_allMethods.csv', index=False, sep=',')\n",
    "result.round(decimals=3).to_csv(output_path + 'PredictionTask_results_allMethods_round.csv', index=False, sep=',')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_map_plot(output_path, method, show_map = False):\n",
    "    \n",
    "    data_shp = gpd.read_file(output_path + method + '/' + method + '_shape.shp')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,15))\n",
    "    data_shp.plot(column='cvd_mortal', ax = ax, legend = True, legend_kwds={'loc': 'lower right'}, cmap = plt.cm.get_cmap('magma_r'), scheme='user_defined', classification_kwds={'bins':[3, 4, 5, 6, 8, 10]})\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(method, fontsize = 20)\n",
    "    plt.savefig(output_path + method + '/' + method + '_plot')\n",
    "    if (show_map == True):\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in methods:\n",
    "#     single_map_plot(output_path, i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_maps_plot(output_path, methods, number_of_columns, color_map = 'YlGn', file_name = 'all_methods_maps',\n",
    "                  bins = None, output_var = None, legend_loc = None):\n",
    "           \n",
    "    # Subplots are organized in a Rows x Cols Grid\n",
    "    # Tot and Cols are known\n",
    "    Tot = len(methods)\n",
    "    Cols = number_of_columns\n",
    "\n",
    "    # Compute Rows required\n",
    "    Rows = Tot // Cols \n",
    "    Rows += Tot % Cols\n",
    "\n",
    "    # Create a Position index\n",
    "    Position = range(1,Tot + 1)\n",
    "    \n",
    "    # Set the size of the figure\n",
    "    fig_size_width = 5 * min(number_of_columns, len(methods))\n",
    "    fig_size_heigth = 7 * np.ceil((len(methods) / number_of_columns))\n",
    "    fig = plt.figure(1, figsize=(fig_size_width, fig_size_heigth))\n",
    "    print(fig_size_width, fig_size_heigth)\n",
    "    \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    \n",
    "    for k in range(Tot):\n",
    "        data_shp = gpd.read_file(output_path + methods[k] + '/' + methods[k] + '_shape.shp')\n",
    "        # if not defined, set the output variable to group the maps by\n",
    "        if not output_var:\n",
    "            for i in data_shp.columns:\n",
    "                if 'mortal' in i:\n",
    "                    output_var = i\n",
    "        # add every single subplot to the figure with a for loop\n",
    "        ax = fig.add_subplot(Rows,Cols,Position[k])\n",
    "        \n",
    "        if (legend_loc == 'bottom'):\n",
    "            if (k == (len(range(Tot)) - 1) and bins):\n",
    "                data_shp.plot(column = output_var, ax = ax, legend = True, legend_kwds={'loc': 'lower right',\n",
    "                              'bbox_to_anchor': (1, 0)}, cmap = plt.cm.get_cmap(color_map), scheme='user_defined',\n",
    "                              classification_kwds={'bins': bins})\n",
    "            elif (k == (len(range(Tot)) - 1) and not bins):\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "                data_shp.plot(column = output_var, ax = ax, legend = True, cax = cax, cmap = plt.cm.get_cmap(color_map))\n",
    "            elif (k < (len(range(Tot)) - 1) and bins):\n",
    "                data_shp.plot(column = output_var, ax = ax, cmap = plt.cm.get_cmap(color_map), scheme='user_defined',\n",
    "                              classification_kwds={'bins':bins})\n",
    "            else:\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "                data_shp.plot(column = output_var, ax = ax, cax = cax, legend = True, cmap = plt.cm.get_cmap(color_map))\n",
    "        else:\n",
    "            if (k == 0 and bins): \n",
    "                data_shp.plot(column = output_var, ax = ax, legend = True, legend_kwds={'loc': 'lower right',\n",
    "                              'bbox_to_anchor': (0, 1)}, cmap = plt.cm.get_cmap(color_map), scheme='user_defined',\n",
    "                              classification_kwds={'bins': bins})\n",
    "            elif (k == 0 and not bins):\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "                data_shp.plot(column = output_var, ax = ax, legend = True, cax = cax, cmap = plt.cm.get_cmap(color_map))\n",
    "            elif (k > 0 and bins):\n",
    "                data_shp.plot(column = output_var, ax = ax, cmap = plt.cm.get_cmap(color_map), scheme='user_defined', classification_kwds={'bins':bins})\n",
    "            else:\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "                data_shp.plot(column = output_var, ax = ax, cax = cax, legend = True, cmap = plt.cm.get_cmap(color_map))\n",
    "            \n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(methods[k].replace('diff_', '') , fontsize= 16)\n",
    "    \n",
    "    plt.savefig(output_path + file_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GT = pd.read_csv(output_path + 'GT/GT.csv', sep=',')\n",
    "merged_data_GT = grid_5km_shp.merge(GT, left_on=\"id\", right_on=\"id\")\n",
    "merged_data_GT.to_file(driver = 'ESRI Shapefile', filename= output_path + 'GT/GT_shape.shp')\n",
    "\n",
    "to_plot = ['GT', 'LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'KNN', 'SVM', 'RF', 'AdaB', 'XGBoost', 'MLP']\n",
    "# all_maps_plot(output_path, to_plot, number_of_columns= 4, color_map= 'magma_r', file_name= 'Plots/all_methods_maps_4col',\n",
    "#               bins= [0.2, 0.4, 0.6, 0.8, 1], output_var='cvd_mortal')\n",
    "# all_maps_plot(output_path, to_plot, number_of_columns = 4, color_map= 'magma_r', file_name= 'Plots/all_methods_maps_4col',\n",
    "#               bins= [-2, -1, 0, 1, 2], output_var='cvd_mortal') \n",
    "all_maps_plot(output_path, to_plot, number_of_columns= 4, color_map= 'magma_r', file_name= 'Plots/all_methods_maps_4col',\n",
    "            bins= [3, 4, 5, 6, 8, 10], output_var='cvd_mortal') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_to_plot = ['LR', 'LR_Ridge', 'LR_Lasso', 'LR_Elastic', 'GAM', 'KNN', 'SVM', 'RF', 'AdaB', 'XGBoost', 'MLP']\n",
    "for i in range(len(methods_to_plot)):\n",
    "    if methods_to_plot[i] != 'GT':\n",
    "        methods_to_plot[i] = 'diff_' + methods_to_plot[i]\n",
    "# all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps_legend',\n",
    "#               bins = [-0.25, 0, 0.25, 0.5], output_var='cvd_mortal', legend_loc = 'bottom')\n",
    "# all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps',\n",
    "#                 bins = [-1, 0, 1, 2.5], output_var='cvd_mortal')\n",
    "all_maps_plot(output_path, methods_to_plot, 4, color_map= 'bwr', file_name= 'Plots/diff_all_methods_maps',\n",
    "              bins = [-1, 0, 1, 2.5, 3.5], output_var='cvd_mortal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make performance correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# sns.set(style=\"ticks\", color_codes=True)\n",
    "# sns.pairplot(preds, corner=True, kind=\"reg\", plot_kws={'line_kws':{'color':'red'}})\n",
    "# plt.savefig(output_path + 'Plots/Performance_correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = preds.corr(method='spearman')\n",
    "# upper_corr_matrix = np.tril(correlation_matrix)\n",
    "# plt.figure()\n",
    "# sns.heatmap(correlation_matrix, cbar=True, fmt='.1f', annot=True, cmap='Reds', mask = upper_corr_matrix )\n",
    "# plt.savefig(output_path + 'Plots/Performance_correlation_Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_colname(x, **kws):\n",
    "  ax = plt.gca()\n",
    "  ax.annotate(x.name, xy=(0.05, 0.9), xycoords=ax.transAxes,\n",
    "              fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrdot(*args, **kwargs):\n",
    "    corr_r = args[0].corr(args[1], 'spearman')\n",
    "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    marker_size = abs(corr_r) * 10000\n",
    "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
    "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
    "    font_size = abs(corr_r) * 40 + 5\n",
    "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
    "                ha='center', va='center', fontsize=font_size)\n",
    "plt.figure()\n",
    "sns.set(style='white', font_scale=2.5)\n",
    "g = sns.PairGrid(preds, aspect=1.4, diag_sharey=False)\n",
    "#g.map_lower(sns.regplot, lowess=True, ci=True, line_kws={'color': 'red'})\n",
    "g.map_lower(sns.regplot, scatter_kws={'s':10}, line_kws={'color': 'red'})\n",
    "g.map_diag(sns.distplot, kde_kws={'color': 'red'})\n",
    "g.map_diag(annotate_colname)\n",
    "g.map_upper(corrdot)\n",
    "\n",
    "plt.savefig(output_path + 'Plots/scatter_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
